import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense

class SelfAttention(Layer):
    def __init__(self, units, **kwargs):
        self.units = units
        super(SelfAttention, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.w_query = self.add_weight(name='query', shape=(input_shape[-1], self.units),
                                       initializer='glorot_uniform', trainable=True)
        self.w_key = self.add_weight(name='key', shape=(input_shape[-1], self.units),
                                     initializer='glorot_uniform', trainable=True)
        self.w_value = self.add_weight(name='value', shape=(input_shape[-1], self.units),
                                       initializer='glorot_uniform', trainable=True)
        super(SelfAttention, self).build(input_shape)
    
    def call(self, inputs):
        query = tf.matmul(inputs, self.w_query)
        key = tf.matmul(inputs, self.w_key)
        value = tf.matmul(inputs, self.w_value)
        
        attention_weights = tf.nn.softmax(tf.matmul(query, key, transpose_b=True))
        output = tf.matmul(attention_weights, value)
        
        return output

# Test the SelfAttention layer
input_data = tf.random.uniform((32, 10, 64))  # (batch_size, sequence_length, input_dim)
attention_layer = SelfAttention(units=8)
output = attention_layer(input_data)
print(output.shape)  # Expected output shape: (32, 10, 8)
